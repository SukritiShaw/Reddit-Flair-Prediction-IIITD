{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDDIT DATA SCRAPING\n",
    "## Modules required to be installed and imported\n",
    "#### PRAW is a Python Reddit API Wrapper that helps in scraping data from Reddit. It needs Client ID and Client Secret to access Redditâ€™s API as a script application while Pandas is being used for converting into dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Declarations\n",
    "#### There are 11 flairs under r/india, for each of those 80 posts are being scraped. The 4 most relevant attributes which will help us further are chosen to be sraped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs_list = [\"AskIndia\", \"Non-Political\", \"Coronavirus\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\"]\n",
    "attributes = {\"flair\":[], \"title\":[], \"url\":[],\"body\":[], \"comments\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data from Reddit\n",
    "#### The scraped data is being stored into a dictionary of lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='##', client_secret='##', user_agent='Scrapping Reddit_data')\n",
    "subreddit = reddit.subreddit('india')\n",
    "for flair in flairs_list:\n",
    "  \n",
    "  get_subreddits = subreddit.search(flair, limit= 80)\n",
    "  \n",
    "  for submission in get_subreddits:\n",
    "    \n",
    "    attributes[\"flair\"].append(flair)\n",
    "    attributes[\"title\"].append(submission.title)\n",
    "    attributes[\"url\"].append(submission.url)\n",
    "    attributes[\"body\"].append(submission.selftext)\n",
    "    \n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "      comment = comment + ' ' + top_level_comment.body\n",
    "    attributes[\"comments\"].append(comment)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion into Dataframe and CSV file.\n",
    "#### The dictionary is then converted into a dataframe and then to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.DataFrame(attributes)\n",
    "attributes.to_csv('Reddit_India_Data.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
