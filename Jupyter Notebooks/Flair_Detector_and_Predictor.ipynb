{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDDIT FLAIR DETECTOR AND PREDICTOR\n",
    "## Modules required to be installed and imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the CSV file and combining data for training and testing\n",
    "#### Another column has been added to the CSV file, namely - COMBINED, where each row contains  the title of the flair as well as the comments of those flairs. This is mainly because after detailed EDA, I inferred that the body includes a lot of verbiage and also  if the body for every flair was used in constructing the model it would lead to lowering the accuracy and also add a lot of latency to the process. The url of the flair contained too many bad symbols and hence, would not help a lot in detection. Therefore, I omitted the URL in the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data = pd.read_csv('Reddit_India_Data.csv')\n",
    "posts_data['COMBINED']= posts_data['title'] + posts_data['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Model used for training and testing\n",
    "#### After analysing a few other  classification models, Linear Support Vector Machine was found the best fitted model on the data. The classification report is also being displayed with respect to individal flairs. The model has been stored as  a .pkl for further use in the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "  model = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "                 ])\n",
    "  model.fit(X_train, y_train)\n",
    "  pickle.dump(model, open('model.pkl', 'wb')) \n",
    "  \n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing of data for Flair Prediction\n",
    "#### The train test split is taken as 70% and 30% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X,y):\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)\n",
    "  print(\"Results of Linear Support Vector Machine Classifier\")\n",
    "  linear_svm(X_train, X_test, y_train, y_test)\n",
    "\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"Coronavirus\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\"]\n",
    "\n",
    "output = posts_data.flair\n",
    "Combined_Input = posts_data.COMBINED\n",
    "Comments_Input = posts_data.comments\n",
    "Title_Input = posts_data.title\n",
    "Y = posts_data.body\n",
    "Z = posts_data.url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Flair Prediction \n",
    "#### Loading the pickle file and detecting the r/India posts according to URLs. After fetching the urls, they undergo the same procedure of conversion into strings and cleaning, for text pre-processing. Prediction of these URLs according to their respective flairs take place after the urls run through the model. Prediction of 2 posts have been limited at a time due to Reddit's scraping rule of not more than 2-3 in a go. Predicted flairs are printed along with their actual flairs to authenticate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_symbols = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "delete_symbols = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = space_symbols.sub(' ', text)\n",
    "    text = delete_symbols.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "reddit = praw.Reddit(client_id='3SYchvDH__igSg', client_secret='ZT-laFvDosSedAE_qPJ0dnkGtdA', user_agent='Scrapping Reddit_data')\n",
    "loaded_model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "def detect_flair(url):\n",
    "    url = str(url)\n",
    "    submission = reddit.submission(url = url)\n",
    "    abcd = {\"title\": [], \"comments\": []}\n",
    "    abcd['title'] = submission.title\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "      comment = comment + ' ' + top_level_comment.body\n",
    "    abcd[\"comments\"].append(comment)\n",
    "    abcd = pd.DataFrame(abcd)\n",
    "    abcd['title'] = abcd['title'].astype(str)\n",
    "    abcd['title'] =abcd['title'].apply(cleaning)\n",
    "    abcd['comments'] = abcd['comments'].astype(str)\n",
    "    abcd['comments'] =abcd['comments'].apply(cleaning)\n",
    "    abcd['com'] = abcd['title'] + abcd['comments'] \n",
    "    return loaded_model.predict(abcd['com'])[0]  \n",
    "\n",
    "subreddit = reddit.subreddit('india')\n",
    "\n",
    "for submission in subreddit.top(limit=2):\n",
    "    print(detect_flair(submission.url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
